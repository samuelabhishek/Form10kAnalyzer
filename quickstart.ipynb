{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python395jvsc74a57bd09023dabb6615afe19dab48841c6bbcfa5de5cb39a9c30d712998d8f94920de2b",
   "display_name": "Python 3.9.5 64-bit ('marketintelligence': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import sec_edgar_downloader\n",
    "import bs4\n",
    "from spacy import matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = sec_edgar_downloader.Downloader(r\"C:\\Users\\samuelabhi\\Documents\\pythonProjects\\Market Intelligence\\documentRepo\")\n",
    "dl.get(\"10-K\", \"MSFT\", amount=1)\n",
    "f = open(r\"C:\\Users\\samuelabhi\\Documents\\pythonProjects\\Market Intelligence\\documentRepo\\sec-edgar-filings\\MSFT\\10-K\\0001564590-20-034944\\filing-details.html\", 'r', encoding = 'utf-8')\n",
    "htmldoc = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(htmldoc, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "64217"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "section = 'Risk Factors'\n",
    "\n",
    "fullText = str(soup.get_text)\n",
    "\n",
    "## Option 1 Identify boundary section IDs from Table of Contents \n",
    "\n",
    "startLink = soup.find_all('a', string = re.compile(section))[0]['href']\n",
    "\n",
    "startSectionID = startLink[startLink.rfind('/')+2:]\n",
    "\n",
    "endLink = [soup.find_all('a')[i+1]['href'] for i, link in enumerate(soup.find_all('a')) if link.string == section]\n",
    "\n",
    "endSectionID = endLink[0][endLink[0].rfind('/')+2:]\n",
    "\n",
    "startText = str(soup.find(id=startSectionID))\n",
    "endText = str(soup.find(id=endSectionID))\n",
    "\n",
    "startIndex = re.search(str(startText), str(fullText)).end()\n",
    "endIndex = re.search(str(endText), str(fullText)).start()\n",
    "\n",
    "extractedHtmlSection = fullText[startIndex:endIndex]\n",
    "\n",
    "extractedSoup = BeautifulSoup(extractedHtmlSection, 'html.parser')\n",
    "\n",
    "extractedText = extractedSoup.get_text()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A reliable way to get the risk headings but some of the section text is missing\n",
    "\n",
    "extractedSubSections = [i.get_text() for i in extractedSoup.find_all('p', style = re.compile(r\"font-weight:bold\"))]\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "docsExtracted = list(nlp.pipe(extractedSubSections))\n",
    "\n",
    "headers = [list(doc.sents)[0] for doc in docsExtracted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Going back to the extracted text to pick up sections for each header\n",
    "\n",
    "doc = nlp(extractedText)\n",
    "\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "headerPatterns = [nlp.make_doc(text.text) for text in headers]\n",
    "\n",
    "matcher.add(\"10KSection\", headerPatterns)\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "a, b, prevEnd = matches[0]\n",
    "\n",
    "sections = []\n",
    "\n",
    "for _, start, end in matches[1:]:\n",
    "    if (start - prevEnd) > 5:\n",
    "        sections.append(doc[prevEnd:start])\n",
    "    else:\n",
    "        sections.append(doc[0:0])\n",
    "    prevEnd = end\n",
    "    \n",
    "sections.append(doc[prevEnd:])\n",
    "\n",
    "insights = zip(headers, sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<zip at 0x15ef22a7ec0>"
      ]
     },
     "metadata": {},
     "execution_count": 95
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ]
}